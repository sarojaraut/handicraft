Excel parsing
    Create a log table to list : file id, file name, file content, date-time of upload
    Parse the excel file and load eachtab to corresponding tables
    Excel column name and table column name can be mapped

create or replace directory blob_dir as '/files';

docker cp C:\Users\sbrn521\Desktop\Files\Clearing_Reference_Data_2018_v3.xlsx oracle-apex:/files

create table data_upload_log (
    id           number generated always as identity, -- generated by default as identity or generated by default on null as identity
    file_name    varchar2(100),
    file_content blob,
    upload_dt    date
);

-- SELECT table_name,
--        column_name,
--        generation_type,
--        identity_options
-- FROM   all_tab_identity_cols;

create or replace procedure load_blob (
    i_dir       in     varchar2,
    i_filename  in     varchar2,
    io_id       in out number
)
is
    l_bfile       bfile;
    l_blob        blob;
    l_dest_offset integer := 1;
    l_src_offset  integer := 1;
begin
    l_bfile := BFILENAME(i_dir, i_filename);
    IF (dbms_lob.fileexists(l_bfile) = 1) THEN
        insert into data_upload_log
        (
            file_name
            ,file_content
            ,upload_dt
        )
        values (
            i_filename
            ,empty_blob()
            ,sysdate
        ) 
        return id, file_content
        into io_id, l_blob;
        dbms_lob.fileopen(l_bfile, dbms_lob.file_readonly );
        dbms_lob.loadfromfile( l_blob, l_bfile, dbms_lob.getlength(l_bfile) );
        dbms_lob.fileclose(l_bfile );
        dbms_output.put_line('File loaded with id ='||io_id);
        commit;
    else
        dbms_output.put_line('File does not exist');
    end if;

END load_blob;
/

set serveroutput on;
DECLARE
    l_id     number;
BEGIN
    load_blob (  
        i_dir         => 'BLOB_DIR',
        i_filename    => 'Clearing_Reference_Data_2018_v3.xlsx',
        io_id         => l_id);
END;
/

--
-- Discover returns CLOB JSON
--
select 
    wwv_flow_data_parser.discover( 
        p_content         => file_content,
        p_file_name       => file_name,
        p_xlsx_sheet_name => 'sheet2.xml'
    )  as profile_json
from data_upload_log f
where f.id = 21;

--
-- Returns the column name and column data type
--
select
    meta.column_position,
    meta.column_name,
    meta.data_type,
    meta.format_mask,
    meta.decimal_char
from data_upload_log f,
    table(
    apex_data_parser.get_columns(
            wwv_flow_data_parser.discover( 
                p_content         => f.file_content,
                p_file_name       => f.file_name,
                p_xlsx_sheet_name => 'sheet2.xml'
            )        
        )
    ) meta
where f.id = 21;

select 
    sheet_display_name, 
    sheet_file_name,
    sheet_sequence,
    sheet_path
from data_upload_log f,
    table( 
        apex_data_parser.get_xlsx_worksheets( 
            p_content => file_content ) 
    ) p
 where f.id = 21;

--
--     P_CONTENT             the file content to be parsed as a BLOB
--     P_FILE_NAME           the name of the file; only used to derive the file type. 
--     P_ADD_HEADERS_ROW     add the detected attribute names for XML or JSON files as the first row
--     P_XLSX_SHEET_NAME     For XLSX workbooks. The name of the worksheet to parse. If omitted or NULL, the function will
--                           use the first worksheet found.
--      
select 
    line_number, 
    col001, 
    col002, 
    col003
from data_upload_log f, 
    table( 
        apex_data_parser.parse(
            p_content                     => f.file_content,
            p_add_headers_row             => 'Y',
            p_xlsx_sheet_name             => 'sheet2.xml',
            p_max_rows                    => 500,
            p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
            p_file_name                   => f.file_name 
        ) 
    )p
where f.id = 21;

--
-- Same as discover but works only after actual parse call
--
select wwv_flow_data_parser.get_file_profile from dual;
--
-- Returns file type based on extension 
--
-- subtype t_file_type is pls_integer range 1..4;
-- c_file_type_xlsx              constant t_file_type      := 1;
-- c_file_type_csv               constant t_file_type      := 2;
-- c_file_type_xml               constant t_file_type      := 3;
-- c_file_type_json              constant t_file_type      := 4;
select apex_data_parser.get_file_type( 'test.xml' ) from dual; -- Returns file type based on extension 

with ws as (
select
    'COURSE,QUALIFICATION,QUAL_OPTION,SUBJECT,COURSE_SUBJECT,QUAL_SUBJECT' names
from dual
)
select regexp_substr(ws.names,'[^,]+',1,rownum)
from ws
connect by rownum <= regexp_count(ws.names,',');

-- File types : Exclusive list of worksheets, all worksheets should be present
-- Mapping table to store source worksheet name and target table name
-- Mapping table to store source column name and target column name
-- Code to build the insert statement
-- Any other data validations
-- Data backup before loading

create or replace package ref_data_loader
is 

end;
/


create or replace package body ref_data_loader
is 
    
end;
/

Fixed partition : three partitions . 
Four partitions : 1. Current active version 2. Last working copy. 3. Gold backup
when you upload a file it first replaces 2 with 1 and then replace excel content with 1.
Anytime user can mark current copy as gold version and then current version will be replaced with gold copy.
Any time user can restore from either last back up or gold copy.

Exchange partition 

function to create a hash value for all column of that table: apex function to use the difference based on primary key
Count mismatch, extra absent and value difference

select 
    line_number, 
    col001, 
    col002, 
    col003,
    col004,
    col005,
    col006,
    col007,
    col008,
    col009,
    col010,
    col011,
    col012,
    col013,
    col014,
    col015,
    col016,
    col017
from data_upload_log f, 
        table( 
            apex_data_parser.parse(
                p_content                     => f.file_content,
                p_add_headers_row             => 'Y',
                p_xlsx_sheet_name             => 'sheet1.xml',
--                p_max_rows                    => 500,
                p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
                p_file_name                   => f.file_name 
            ) 
        ) data
where f.id = 21
and line_number <> 1;

select 
    line_number, 
    col001, 
    col002, 
    col003
from data_upload_log f, 
        table( 
            apex_data_parser.parse(
                p_content                     => f.file_content,
                p_add_headers_row             => 'Y',
                p_xlsx_sheet_name             => 'sheet2.xml',
--                p_max_rows                    => 500,
                p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
                p_file_name                   => f.file_name 
            ) 
        ) data
where f.id = 21
and line_number <> 1;

select 
    line_number, 
    col001, 
    col002, 
    col003,
    col004,
    col005
from data_upload_log f, 
        table( 
            apex_data_parser.parse(
                p_content                     => f.file_content,
                p_add_headers_row             => 'Y',
                p_xlsx_sheet_name             => 'sheet3.xml',
--                p_max_rows                    => 500,
                p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
                p_file_name                   => f.file_name 
            ) 
        ) data
where f.id = 21
and line_number <> 1;


select 
    line_number, 
    col001, 
    col002
from data_upload_log f, 
        table( 
            apex_data_parser.parse(
                p_content                     => f.file_content,
                p_add_headers_row             => 'Y',
                p_xlsx_sheet_name             => 'sheet4.xml',
                p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
                p_file_name                   => f.file_name 
            ) 
        ) data
where f.id = 21
and line_number <> 1;

select 
    line_number, 
    col001, 
    col002, 
    col003,
    col004,
    col005
from data_upload_log f, 
        table( 
            apex_data_parser.parse(
                p_content                     => f.file_content,
                p_add_headers_row             => 'Y',
                p_xlsx_sheet_name             => 'sheet5.xml',
                p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
                p_file_name                   => f.file_name 
            ) 
        ) data
where f.id = 21
and line_number <> 1;

select 
    line_number, 
    col001, 
    col002, 
    col003
from data_upload_log f, 
        table( 
            apex_data_parser.parse(
                p_content                     => f.file_content,
                p_add_headers_row             => 'Y',
                p_xlsx_sheet_name             => 'sheet6.xml',
                p_store_profile_to_collection => 'FILE_PARSER_COLLECTION',
                p_file_name                   => f.file_name 
            ) 
        ) data
where f.id = 21
and line_number <> 1;

insert into working partitions 

range 
list
hash 
interval
