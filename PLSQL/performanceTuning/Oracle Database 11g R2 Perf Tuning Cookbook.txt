Oracle Database 11g R2 Perf Tuning Cookbook

To transfer statistics between different databases, we have to use a statistics table, as shown in the following steps:\

1. Create the statistics table on the source database.
2. Export the statistics from the data dictionary to the statistics table.
3. Move the statistics table (Export/Import, Datapump, Copy) to the target database.
4. Import the statistics from the statistics table to the data dictionary.
5. Drop the statistics table.

The corresponding statements to execute on the source database are as follows:
EXEC DBMS_STATS.create_stat_table('DBA_SCHEMA', 'MY_STAT_TABLE');
EXEC DBMS_STATS.export_schema_stats('DBA_SCHEMA', 'MY_STAT_TABLE', NULL,'APP_SCHEMA');

With these statements we have created the statistics table MY_STAT_TABLE in the DBA_SCHEMA and populated it with data from the APP_SCHEMA (for example, HR).

Then we transfer the MY_STAT_TABLE to the target database; using the export/import command line utilities we export the table from source database and then import the table into the target database, in which we execute the following statements:
EXEC DBMS_STATS.import_schema_stats('APP_SCHEMA', 'MY_STAT_TABLE', NULL,'DBA_SCHEMA');
EXEC DBMS_STATS.drop_stat_table('DBA_SCHEMA', 'MY_STAT_TABLE');

In the example, we have transferred statistics about the entire schema APP_SCHEMA. We can choose to transfer statistics for the entire database, a table, an index, or a column, using the corresponding import_* and export_* procedures of the DBMS_STATS package. 

Demerits of dynamic sql

The code is more prone to bugs - It could take long to test every SQL statement generated by our procedures.
The database can't check dependencies for dynamic SQL
Tuning a dynamic SQL procedure can be difficult - because the actual query is built at runtime.

Optimizing Storage Structures

Row Chaning : We encounter row chaining when the size of the row data is larger than the size of the database block used to store it. In this situation, the row is split across more than one database block, so, to read it we need to access more than one database block, resulting in greater I/O.

CREATE TABLE HR.BIG_ROWS (
id number NOT NULL,
field1 char(2000) DEFAULT 'A' NOT NULL,
field2 char(2000) DEFAULT 'B' NOT NULL,
field3 char(2000) DEFAULT 'C' NOT NULL,
field4 char(2000) DEFAULT 'D' NOT NULL,
field5 char(2000) DEFAULT 'E' NOT NULL,
constraint PK_BIG_ROWS primary key (ID))
TABLESPACE EXAMPLE;

INSERT INTO HR.BIG_ROWS (id) select rownum from all_objects where rownum < 101;

ANALYZE TABLE HR.BIG_ROWS COMPUTE STATISTICS;

SELECT CHAIN_CNT FROM ALL_TABLES WHERE OWNER = ‹HR› AND TABLE_NAME = ‹BIG_ROWS›;

SELECT CHAIN_CNT FROM ALL_TABLES WHERE OWNER = ‹HR› AND TABLE_NAME = ‹BIG_ROWS›;

CREATE TABLESPACE TS_16K BLOCKSIZE 16K DATAFILE 'TS_16K.DBF' SIZE 10M EXTENT MANAGEMENT LOCAL UNIFORM SIZE 1M;

ALTER SYSTEM SET db_16k_cache_size = 16m scope=both;

ALTER TABLE HR.BIG_ROWS MOVE TABLESPACE TS_16K;

ALTER INDEX HR.PK_BIG_ROWS REBUILD;

ANALYZE TABLE HR.BIG_ROWS COMPUTE STATISTICS;

SELECT CHAIN_CNT FROM ALL_TABLES WHERE OWNER = ‹HR› AND TABLE_NAME = ‹BIG_ROWS›;

DROP TABLESPACE TS_16K INCLUDING CONTENTS AND DATAFILES;


Before adding a tablespace with a different DB block size, we have to make room in the database buffer cache to store DB blocks of every size. when having tablespaces with
different DB block sizes in the database—as we have seen— we have to reserve space in the database buffer for a different DB block size, and the memory reserved to a particular block size cannot be used for caching database blocks of a different size. This situation led to a possible waste of memory. For example, we have 512 MB reserved for db_16k_cache_size unused, because we have few objects stored with this database block size, and the db_8k_ cache_size is fully utilized. Tuning the buffer cache can become a nightmare in such an environment.

Avoiding row migration : When we update a row and it does not fit entirely within the original database block due to the corresponding growth in size, we have a row migration. In the original place (where the row was stored) we have placed a pointer to the new location of the row. 

CREATE TABLE HR.BIG_ROWS (
id number NOT NULL,
field1 char(2000) DEFAULT 'A' NOT NULL,
field2 char(2000),
field3 char(2000),
field4 char(1000),
constraint PK_BIG_ROWS primary key (ID))
TABLESPACE EXAMPLE PCTFREE 10;

INSERT INTO HR.BIG_ROWS (id) select rownum from all_objects where rownum < 101;

ANALYZE TABLE HR.BIG_ROWS COMPUTE STATISTICS;

SELECT CHAIN_CNT FROM ALL_TABLES WHERE OWNER = ‹HR› AND TABLE_NAME = ‹BIG_ROWS›;

--CHAIN_CNT = 0;

UPDATE HR.BIG_ROWS SET field2 = 'B', field3 = ‹C›, field4 = ‹D› WHERE MOD(id, 2) = 1;

ANALYZE TABLE HR.BIG_ROWS COMPUTE STATISTICS;

SELECT CHAIN_CNT FROM ALL_TABLES WHERE OWNER = ‹HR› AND TABLE_NAME = ‹BIG_ROWS›;

--CHAIN_CNT = 50;

create table HR.CHAINED_ROWS (
owner_name varchar2(30),
table_name varchar2(30),
cluster_name varchar2(30),
partition_name varchar2(30),
subpartition_name varchar2(30),
head_rowid rowid,
analyze_timestamp date
);

ANALYZE TABLE HR.BIG_ROWS LIST CHAINED ROWS INTO HR.CHAINED_ROWS;

SELECT COUNT(*) FROM HR.CHAINED_ROWS;

CREATE TABLE TEMP_BIG_ROWS AS SELECT * FROM HR.BIG_ROWS WHERE 1=0;

INSERT INTO TEMP_BIG_ROWS SELECT B.* 
FROM HR.BIG_ROWS B, HR.CHAINED_ROWS T
WHERE T.OWNER_NAME = ‹HR› AND T.TABLE_NAME = ‹BIG_ROWS›
AND T.HEAD_ROWID = B.ROWID;

DELETE FROM HR.BIG_ROWS B WHERE EXISTS (
SELECT T.ROWID FROM HR.CHAINED_ROWS T
WHERE T.OWNER_NAME = ‹HR› AND T.TABLE_NAME = ‹BIG_ROWS›
AND T.HEAD_ROWID = B.ROWID);

INSERT INTO HR.BIG_ROWS SELECT * FROM HR.TEMP_BIG_ROWS;

ANALYZE TABLE HR.BIG_ROWS COMPUTE STATISTICS;

SELECT CHAIN_CNT FROM ALL_TABLES WHERE OWNER = 'HR' AND TABLE_NAME = ‹BIG_ROWS›;

--CHAIN_CNT = 0;

DROP TABLE HR.TEMP_BIG_ROWS;
DROP TABLE HR.CHAINED_ROWS;
DROP TABLE HR.BIG_ROWS;

To prevent row migration, we have to set a higher value for the table PCTFREE storage parameters, to allow more space in the block, which is free for subsequent updates of the rows.

Rebuilding an index is an operation that can provide performance benefits because it reduces intra-block fragmentation. 

ALTER INDEX IX1_MYCUSTOMERS REBUILD ONLINE PARALLEL;

If we don't specify the ONLINE clause, then any DML operations will wait until the completion of rebuild process.

A reverse key B-tree index stores the value in the leaf nodes reversed (from right to left, instead of from left to right); for example, the value 123 will be stored in the index as 321. the index is spread across more database blocks, occupying more space on the disk. This is the exact behavior of this kind of index—distributing index key values in different database blocks. This feature can improve performance, because where we have a table with a field populated by a sequence, massive inserts from multiple sessions could lead to contention issues on the index blocks. In fact, every session tries to insert new values in the table. Because the key values are produced by a sequence, the corresponding index entries will be adjacent in the leaf nodes of the B-tree index (for example, the values 123, 124, 125, and so on). Storing key values in reverse (for example, in the previous example the key values will be stored as 321, 421, 521, and so on) led to spreading the values in different leaf nodes of the index, reducing contention issues.

Create a BITMAP INDEX on some columns:
CREATE BITMAP INDEX BIX_GENDER_MARITAL_YOB ON MYCUSTOMERS (CUST_GENDER, CUST_MARITAL_STATUS, CUST_YEAR_OF_BIRTH);
